{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ea8e185-07bc-42a9-a032-802863254f7a",
   "metadata": {},
   "source": [
    "# Reinforcement Learning for Navigation in Grid-Based Environments: Dynamic Programming (Value Iteration & Policy Iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7905f03c-e76c-4c1f-9f9f-62a66ab459cc",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c1bae0-ffbb-4c95-bcaf-9cd1b3c9bb2e",
   "metadata": {},
   "source": [
    "This project is literally a clone of a previous one, where we used Q-Learning to teach an agent how to move in a 5 x 5 grid world to reach a goal while avoiding obstacles.\n",
    "However, this time we are using a model-based reinforcement learning approach, specifically Dynamic Programming (DP), instead of a model-free one like Q-Learning.\n",
    "\n",
    "**The main difference between these two approaches is:**\n",
    "\n",
    "| Concept | Q-Learning | Dynamic Programming |\n",
    "| ------- | ---------- | ------------------- |\n",
    "| Type    | Model-Free RL | Model-Based RL          |\n",
    "| Requires knowledge of environment?       | No   | yes  |\n",
    "| Learn by interation?       | Yes (explores & learns) | No (plans everything) |\n",
    "| Computes       | Q-values         | Value function and optimal policy |\n",
    "| Behavior       | Learns gradually from experience         | Computes optimal plan directly |\n",
    "\n",
    "In short, Q-Learning makes the agent learn from experience, so it doesn't know what will happen until it tries. In the other hand, Dynamic Programming already knows how the environement works (the transition and rewards), so it can calculate the optimal policy directly before any movement happens.\n",
    "\n",
    "In this project, we will implement **Value Iteration** and **Policy Iteration**, two key algorithms used in DP."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc62fcf-4d6c-4d84-a153-bb428a333768",
   "metadata": {},
   "source": [
    "## Understanding Dynamic Programming in Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f527addd-00c1-4995-b458-472da3658e69",
   "metadata": {},
   "source": [
    "Dynamic Programming (DP) is a family of algorithms that solve complex problems by breaking them into smaller overlapping subproblems and solving each subproblem recursively.\n",
    "\n",
    "In Reinforcement Learning, DP methods are used to compute optimal policies and value functions when we have a complete model of the environment; that is, we know the transition probabilities P(s'/s,a) and rewards R(s,a).\n",
    "\n",
    "it's aim is to find:\n",
    "* The optimal value function V*(s) or Q*(s,a)\n",
    "* The corresponding optimal policy π*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74fcb6bc-13eb-4ea3-89b8-09f119f39d2e",
   "metadata": {},
   "source": [
    "#### The Principle of Optimality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5b70ea-ee82-49f9-8fa2-266fd0c3235b",
   "metadata": {},
   "source": [
    "Dynamic Programming relies on Bellman’s Principle of Optimality:\n",
    "\n",
    "    An optimal policy has the property that, whatever the initial state and initial decision, the remaining decisions must constitute an optimal policy with respect to the state resulting from the first decision.\n",
    "\n",
    "In simple terms:\n",
    "\n",
    "    “If I make the best choice now, and then continue making the best choices in the future, I’ll be optimal overall.”\n",
    "\n",
    "This recursive nature leads directly to the Bellman optimality equations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197bf82d-397e-42fc-be40-5c59ec979064",
   "metadata": {},
   "source": [
    "#### Main Ideas of Dynamic Programming in RL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2908e8cc-7399-410c-b029-8f6adee62d7c",
   "metadata": {},
   "source": [
    "DP in RL typically involves fours main ideas or steps. Let's go through each:\n",
    "\n",
    "* **Policy Evaluation**\n",
    "The idea here is considering a policy π, and then compute its value function Vπ*(s).\n",
    "We can solve this iteratively by repeatedly updating V(s) until convergence, what is called Iterative Policy Evaluation.\n",
    "In other words, we estimate how good the policy is by computing expected returns for each state under that policy.\n",
    "\n",
    "* **Policy Improvement**\n",
    "Here, given Vπ*(s), we try to make a better policy π' by choosing in each state the action that seems best according to the current value function. This step greedily improves the policy using the most recent value estimates.\n",
    "Otherwise, we use the current knowledge of how good each state is to choose better actions.\n",
    "\n",
    "* **Policy Iteration**\n",
    "In this step we combine Policy Evaluation and Policy Improvement until convergence:\n",
    "We start with a random policy, we Evaluate Vπ, then we improve it to get π', and we repeat until the policy stops changing.\n",
    "\n",
    "* **Value Iteration**\n",
    "This step is crucial to speed up policy iteration by combining evaluation and improvement into a single step. So instead of alternating evaluation and improvement, it just updates values using the Bellman optimality equation until convergence. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf351a4-2e97-4199-b223-45287e6c54cb",
   "metadata": {},
   "source": [
    "#### Limitations of DP in RL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ecd8971-c676-489b-ac8a-c595f1e539f2",
   "metadata": {},
   "source": [
    "Dynamic Programming is foundational but not always practical because it assumes:\n",
    "1. You know the environment model P(s'/s,a) and R(s,a).\n",
    "2. The state and action spaces are small enough to compute and store tables of values.\n",
    "\n",
    "That's why in modern RL, sampling-based methods (like Monte Carlo, TD, and Q-Learning) are used to approximate DP's behavior without knowing the full model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4685f6fa-42cf-43f6-bc9d-e116d21179d2",
   "metadata": {},
   "source": [
    "## Value Iteration Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1edc256a-f16a-4733-8413-97bc19744584",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Iteration Converged in 9 iterations\n",
      "\n",
      "Value Function after Convergence:\n",
      "[[0.21744535 0.2971615  0.385735   0.48415    0.5935    ]\n",
      " [0.2971615  0.385735   0.48415    0.5935     0.715     ]\n",
      " [0.385735   0.48415    0.5935     0.         0.85      ]\n",
      " [0.48415    0.5935     0.715      0.85       1.        ]\n",
      " [0.385735   0.48415    0.         1.         0.        ]]\n",
      "\n",
      "Optimal Policy:\n",
      "[['D' 'D' 'D' 'D' 'D']\n",
      " ['D' 'D' 'D' 'R' 'D']\n",
      " ['D' 'D' 'D' 'X' 'D']\n",
      " ['R' 'R' 'R' 'D' 'D']\n",
      " ['U' 'U' 'X' 'R' 'G']]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the Environment Setup\n",
    "grid_size = (5, 5)\n",
    "goal_state = (4, 4)\n",
    "obstacles= [(4, 2), (2, 3)]\n",
    "actions = ['U', 'D', 'R', 'L']\n",
    "gamma = 0.9\n",
    "epsilon = 1e-4\n",
    "\n",
    "# Define the Reward Function\n",
    "def get_reward(next_state):\n",
    "    if next_state == goal_state:\n",
    "        return 1.0\n",
    "    elif next_state in obstacles or not (0 <= next_state[0] < grid_size[0] and 0 <= next_state[1] < grid_size[1]):\n",
    "        return -1.0\n",
    "    else:\n",
    "        return -0.05\n",
    "    \n",
    "# Define the Transition Model\n",
    "def get_next_state(state, action):\n",
    "    i, j = state\n",
    "    if action == 'U':\n",
    "        next_state = (i -1, j)\n",
    "    elif action == 'D':\n",
    "        next_state = (i + 1, j)\n",
    "    elif action == 'R':\n",
    "        next_state = (i, j + 1)\n",
    "    elif action == 'L':\n",
    "        next_state = (i, j - 1)\n",
    "    else:\n",
    "        next_state = state\n",
    "\n",
    "    if next_state in obstacles or not (0 <= next_state[0] < grid_size[0] and 0 <= next_state[1] < grid_size[1]):\n",
    "        next_state = state\n",
    "    \n",
    "    return next_state\n",
    "\n",
    "# Initialize Value Function\n",
    "V = np.zeros(grid_size)\n",
    "\n",
    "def value_iteration():\n",
    "    iteration = 0\n",
    "    while True:\n",
    "        delta = 0\n",
    "        new_V = np.copy(V)\n",
    "        for i in range(grid_size[0]):\n",
    "            for j in range(grid_size[1]):\n",
    "                state = (i, j)\n",
    "                if state == goal_state or state in obstacles:\n",
    "                    continue\n",
    "\n",
    "                value_list = []\n",
    "                for action in actions:\n",
    "                    next_state = get_next_state(state, action)\n",
    "                    r = get_reward(next_state)\n",
    "                    value = r + gamma * V[next_state]\n",
    "                    value_list.append(value)\n",
    "                new_V[state] = max(value_list)\n",
    "                delta = max(delta, abs(new_V[state] - V[state]))\n",
    "        \n",
    "        V[:] = new_V\n",
    "        iteration += 1\n",
    "        if delta < epsilon:\n",
    "            break\n",
    "    return iteration\n",
    "\n",
    "iterations = value_iteration()\n",
    "\n",
    "# Derive the Optimal Policy\n",
    "policy = np.full(grid_size, ' ')\n",
    "for i in range(grid_size[0]):\n",
    "    for j in range(grid_size[1]):\n",
    "        state = (i, j)\n",
    "        if state == goal_state:\n",
    "            policy[state] = 'G'\n",
    "        elif state in obstacles:\n",
    "            policy[state] = 'X'\n",
    "        else:\n",
    "            best_action = None\n",
    "            best_value = -np.inf\n",
    "            for action in actions:\n",
    "                next_state = get_next_state(state, action)\n",
    "                r = get_reward(next_state)\n",
    "                value = r + gamma * V[next_state]\n",
    "                if value > best_value:\n",
    "                    best_value = value\n",
    "                    best_action = action\n",
    "            policy[state] = best_action\n",
    "\n",
    "print(\"Policy Iteration Converged in\", iterations, \"iterations\")\n",
    "print(\"\\nValue Function after Convergence:\")\n",
    "print(V)\n",
    "print(\"\\nOptimal Policy:\")\n",
    "print(policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ff9cd6-4f4d-4a45-84f8-15ff35a1251d",
   "metadata": {},
   "source": [
    "## Policy Iteration Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ec1f14d-d4db-472e-98cf-99fdf78f78cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Iteration Converged in 8 iterations\n",
      "\n",
      "Final Value Function:\n",
      " [[0.21744535 0.2971615  0.385735   0.48415    0.5935    ]\n",
      " [0.2971615  0.385735   0.48415    0.5935     0.715     ]\n",
      " [0.385735   0.48415    0.5935     0.         0.85      ]\n",
      " [0.48415    0.5935     0.715      0.85       1.        ]\n",
      " [0.385735   0.48415    0.         1.         0.        ]]\n",
      "\n",
      "Optimal Policy:\n",
      " [['D' 'D' 'D' 'D' 'D']\n",
      " ['D' 'D' 'D' 'R' 'D']\n",
      " ['D' 'D' 'D' 'X' 'D']\n",
      " ['R' 'R' 'R' 'D' 'D']\n",
      " ['U' 'U' 'X' 'R' 'G']]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the Environment Setup\n",
    "grid_size = (5, 5)\n",
    "goal_state = (4, 4)\n",
    "obstacles= [(4, 2), (2, 3)]\n",
    "actions = ['U', 'D', 'R', 'L']\n",
    "gamma = 0.9\n",
    "epsilon = 1e-4\n",
    "\n",
    "# Define the Reward Function\n",
    "def get_reward(next_state):\n",
    "    if next_state == goal_state:\n",
    "        return 1.0\n",
    "    elif next_state in obstacles or not (0 <= next_state[0] < grid_size[0] and 0 <= next_state[1] < grid_size[1]):\n",
    "        return -1.0\n",
    "    else:\n",
    "        return -0.05\n",
    "    \n",
    "# Define the Transition Model\n",
    "def get_next_state(state, action):\n",
    "    i, j = state\n",
    "    if action == 'U':\n",
    "        next_state = (i -1, j)\n",
    "    elif action == 'D':\n",
    "        next_state = (i + 1, j)\n",
    "    elif action == 'R':\n",
    "        next_state = (i, j + 1)\n",
    "    elif action == 'L':\n",
    "        next_state = (i, j - 1)\n",
    "    else:\n",
    "        next_state = state\n",
    "\n",
    "    if next_state in obstacles or not (0 <= next_state[0] < grid_size[0] and 0 <= next_state[1] < grid_size[1]):\n",
    "        next_state = state\n",
    "    \n",
    "    return next_state\n",
    "\n",
    "def policy_evaluation(policy, V):\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for i in range(grid_size[0]):\n",
    "            for j in range(grid_size[1]):\n",
    "                state = (i, j)\n",
    "                if state == goal_state or state in obstacles:\n",
    "                    continue\n",
    "                action = policy[state]\n",
    "                next_state = get_next_state(state, action)\n",
    "                r = get_reward(next_state)\n",
    "                new_value = r + gamma * V[next_state]\n",
    "                delta = max(delta, abs(V[state] - new_value))\n",
    "                V[state] = new_value\n",
    "        if delta < epsilon:\n",
    "            break\n",
    "    return V\n",
    "\n",
    "def policy_improvement(V, policy):\n",
    "    policy_stable = True\n",
    "    for i in range(grid_size[0]):\n",
    "        for j in range(grid_size[1]):\n",
    "            state = (i, j)\n",
    "            if state == goal_state or state in obstacles:\n",
    "                continue\n",
    "            old_action = policy[state]\n",
    "            best_action = None\n",
    "            best_value = -np.inf\n",
    "            for action in actions:\n",
    "                next_state = get_next_state(state, action)\n",
    "                r = get_reward(next_state)\n",
    "                value = r + gamma * V[next_state]\n",
    "                if value > best_value:\n",
    "                    best_value = value\n",
    "                    best_action = action\n",
    "            policy[state] = best_action\n",
    "            if old_action != best_action:\n",
    "                policy_stable = False\n",
    "    return policy, policy_stable\n",
    "\n",
    "# Initialize random policy\n",
    "policy = np.random.choice(actions, grid_size)\n",
    "V = np.zeros(grid_size)\n",
    "\n",
    "# Mark goal and obstacle states in policy grid\n",
    "policy[goal_state] = 'G'\n",
    "for obs in obstacles:\n",
    "    policy[obs] = 'X'\n",
    "\n",
    "# Policy Iteration loop\n",
    "iteration = 0\n",
    "while True:\n",
    "    iteration += 1\n",
    "    V = policy_evaluation(policy, V)\n",
    "    policy, stable = policy_improvement(V, policy)\n",
    "    if stable:\n",
    "        break\n",
    "\n",
    "print(\"Policy Iteration Converged in\", iteration, \"iterations\")\n",
    "print(\"\\nFinal Value Function:\\n\", V)\n",
    "print(\"\\nOptimal Policy:\\n\", policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d45b55c-4b6d-4162-aacd-bdce8956b3b1",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65cf7267-b4cd-473b-af6d-d42d46ce561e",
   "metadata": {},
   "source": [
    "In this project, we solved a 5×5 Grid World environment using Dynamic Programming, which is a model-based reinforcement learning approach.\n",
    "\n",
    "Key Takeaways:\n",
    "\n",
    "* Dynamic Programming needs complete knowledge of the environment’s model.\n",
    "* Value Iteration and Policy Iteration are two main DP algorithms for finding optimal policies.\n",
    "* Both algorithms rely on the Bellman equations to compute value functions.\n",
    "* Unlike Q-Learning, which learns through exploration, DP methods plan ahead using a known model.\n",
    "* The optimal policy derived from DP can be visualized to show the best path from the start to the goal.\n",
    "\n",
    "In summary, Dynamic Programming acts like a planner, while Q-Learning acts like a learner.\n",
    "Both aim for the same goal, that is to find the best way for an agent to act, but they use very different paths to get there."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
